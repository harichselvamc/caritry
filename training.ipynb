{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (choose the appropriate command from https://pytorch.org/get-started/locally/)\n",
    "!pip install torch torchvision\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install numpy matplotlib pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CaricatureDataset(Dataset):\n",
    "    def __init__(self, data_file, root_dir, transform=None, split='train', split_ratio=0.8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_file (str): Path to the data.txt file.\n",
    "            root_dir (str): Root directory containing the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            split (str): 'train' or 'val' to indicate dataset split.\n",
    "            split_ratio (float): Ratio to split the dataset into training and validation.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        print(f\"Loading data from {data_file}...\")\n",
    "        # Read the data file\n",
    "        with open(data_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        print(f\"Total data entries found: {len(lines)}\")\n",
    "\n",
    "        # Shuffle the data\n",
    "        random.shuffle(lines)\n",
    "        print(\"Data shuffled.\")\n",
    "\n",
    "        # Split the data\n",
    "        split_idx = int(len(lines) * split_ratio)\n",
    "        if split == 'train':\n",
    "            self.lines = lines[:split_idx]\n",
    "            print(f\"Selected {len(self.lines)} samples for training.\")\n",
    "        else:\n",
    "            self.lines = lines[split_idx:]\n",
    "            print(f\"Selected {len(self.lines)} samples for validation.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Parse the line\n",
    "        line = self.lines[idx].strip().split()\n",
    "        # Assuming the format:\n",
    "        # [Identity] [Caricature Image Path] [Real Image Path] [Label]\n",
    "        # Adjust indices if necessary\n",
    "        caricature_path = line[1].replace('\\\\', '/')\n",
    "        real_path = line[2].replace('\\\\', '/')\n",
    "\n",
    "        # Full paths\n",
    "        caricature_full_path = os.path.join(self.root_dir, caricature_path)\n",
    "        real_full_path = os.path.join(self.root_dir, real_path)\n",
    "\n",
    "        # Debug: Print the paths being loaded\n",
    "        if idx < 5:  # Print first 5 samples\n",
    "            print(f\"Loading sample {idx}:\")\n",
    "            print(f\"  Caricature Image Path: {caricature_full_path}\")\n",
    "            print(f\"  Real Image Path: {real_full_path}\")\n",
    "\n",
    "        # Open images\n",
    "        try:\n",
    "            real_image = Image.open(real_full_path).convert('RGB')\n",
    "            caricature_image = Image.open(caricature_full_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images at index {idx}: {e}\")\n",
    "            # You can choose to skip or handle the error as needed\n",
    "            raise e\n",
    "\n",
    "        if self.transform:\n",
    "            real_image = self.transform(real_image)\n",
    "            caricature_image = self.transform(caricature_image)\n",
    "\n",
    "        return real_image, caricature_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        self.down1 = self.contracting_block(in_channels, features, bn=False)      # 256 -> 128\n",
    "        self.down2 = self.contracting_block(features, features*2)               # 128 -> 64\n",
    "        self.down3 = self.contracting_block(features*2, features*4)             # 64 -> 32\n",
    "        self.down4 = self.contracting_block(features*4, features*8)             # 32 -> 16\n",
    "        self.down5 = self.contracting_block(features*8, features*8)             # 16 -> 8\n",
    "        self.down6 = self.contracting_block(features*8, features*8)             # 8 -> 4\n",
    "        self.down7 = self.contracting_block(features*8, features*8)             # 4 -> 2\n",
    "        self.down8 = self.contracting_block(features*8, features*8, bn=False)   # 2 -> 1\n",
    "\n",
    "        self.up1 = self.expansive_block(features*8, features*8, dropout=0.5)     # 1 -> 2\n",
    "        self.up2 = self.expansive_block(features*16, features*8, dropout=0.5)    # 2 -> 4\n",
    "        self.up3 = self.expansive_block(features*16, features*8, dropout=0.5)    # 4 -> 8\n",
    "        self.up4 = self.expansive_block(features*16, features*8)                 # 8 -> 16\n",
    "        self.up5 = self.expansive_block(features*16, features*4)                 # 16 -> 32\n",
    "        self.up6 = self.expansive_block(features*8, features*2)                  # 32 -> 64\n",
    "        self.up7 = self.expansive_block(features*4, features)                    # 64 -> 128\n",
    "        self.up8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output values between -1 and 1\n",
    "        )  # 128 -> 256\n",
    "\n",
    "    def contracting_block(self, in_channels, out_channels, bn=True):\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        ]\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def expansive_block(self, in_channels, out_channels, dropout=0):\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        u1 = self.up1(d8)\n",
    "        u1 = torch.cat([u1, d7], 1)\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat([u2, d6], 1)\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat([u3, d5], 1)\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat([u4, d4], 1)\n",
    "        u5 = self.up5(u4)\n",
    "        u5 = torch.cat([u5, d3], 1)\n",
    "        u6 = self.up6(u5)\n",
    "        u6 = torch.cat([u6, d2], 1)\n",
    "        u7 = self.up7(u6)\n",
    "        u7 = torch.cat([u7, d1], 1)\n",
    "        u8 = self.up8(u7)\n",
    "\n",
    "        return u8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing datasets...\n",
      "Loading data from C:\\Users\\haric\\Downloads\\cari\\dataset\\data.txt...\n",
      "Total data entries found: 21064\n",
      "Data shuffled.\n",
      "Selected 16851 samples for training.\n",
      "Loading data from C:\\Users\\haric\\Downloads\\cari\\dataset\\data.txt...\n",
      "Total data entries found: 21064\n",
      "Data shuffled.\n",
      "Selected 4213 samples for validation.\n",
      "Datasets initialized.\n",
      "Creating DataLoaders...\n",
      "DataLoaders created.\n",
      "Initializing models...\n",
      "Models initialized.\n",
      "Loss functions and optimizers set.\n",
      "Output directory: outputs\n",
      "Checkpoint directory: checkpoints\n",
      "\n",
      "Starting Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "    # ============================\n",
    "    # Hyperparameters and Paths\n",
    "    # ============================\n",
    "    NUM_EPOCHS = 200\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-4\n",
    "    L1_LAMBDA = 100\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # Paths (Update these paths as per your system)\n",
    "    DATA_FILE = r'C:\\Users\\haric\\Downloads\\cari\\dataset\\data.txt'\n",
    "    ROOT_DIR = r'C:\\Users\\haric\\Downloads\\cari\\dataset'\n",
    "    OUTPUT_DIR = 'outputs'\n",
    "    CHECKPOINT_DIR = 'checkpoints'\n",
    "\n",
    "    # ============================\n",
    "    # Transforms\n",
    "    # ============================\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "    # ============================\n",
    "    # Create Datasets and DataLoaders\n",
    "    # ============================\n",
    "    print(\"Initializing datasets...\")\n",
    "    train_dataset = CaricatureDataset(data_file=DATA_FILE, root_dir=ROOT_DIR, transform=transform, split='train', split_ratio=0.8)\n",
    "    val_dataset = CaricatureDataset(data_file=DATA_FILE, root_dir=ROOT_DIR, transform=transform, split='val', split_ratio=0.8)\n",
    "    print(\"Datasets initialized.\")\n",
    "\n",
    "    print(\"Creating DataLoaders...\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    print(\"DataLoaders created.\")\n",
    "\n",
    "    # ============================\n",
    "    # Initialize Models\n",
    "    # ============================\n",
    "    print(\"Initializing models...\")\n",
    "    generator = UNetGenerator().to(DEVICE)\n",
    "    discriminator = PatchDiscriminator().to(DEVICE)\n",
    "    print(\"Models initialized.\")\n",
    "\n",
    "    # ============================\n",
    "    # Loss Functions and Optimizers\n",
    "    # ============================\n",
    "    criterion_GAN = nn.MSELoss()\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    print(\"Loss functions and optimizers set.\")\n",
    "\n",
    "    # ============================\n",
    "    # Create Output Directories\n",
    "    # ============================\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "    # ============================\n",
    "    # Training Loop\n",
    "    # ============================\n",
    "    def denormalize(tensors):\n",
    "        return (tensors * 0.5) + 0.5\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        epoch_loss_G = 0\n",
    "        epoch_loss_D = 0\n",
    "\n",
    "        print(f\"\\nStarting Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for i, (real_img, caricature_img) in enumerate(train_loader):\n",
    "            real_img = real_img.to(DEVICE)\n",
    "            caricature_img = caricature_img.to(DEVICE)\n",
    "\n",
    "            # ============================\n",
    "            # Train Discriminator\n",
    "            # ============================\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real pairs\n",
    "            real_pair = torch.cat((real_img, caricature_img), 1)\n",
    "            pred_real = discriminator(real_pair)\n",
    "            real_loss = criterion_GAN(pred_real, torch.ones_like(pred_real).to(DEVICE))\n",
    "            # Debug\n",
    "            if i == 0:\n",
    "                print(f\"Batch {i+1}: Real loss: {real_loss.item():.4f}\")\n",
    "\n",
    "            # Fake pairs\n",
    "            fake_img = generator(real_img)\n",
    "            fake_pair = torch.cat((real_img, fake_img.detach()), 1)\n",
    "            pred_fake = discriminator(fake_pair)\n",
    "            fake_loss = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(DEVICE))\n",
    "            # Debug\n",
    "            if i == 0:\n",
    "                print(f\"Batch {i+1}: Fake loss: {fake_loss.item():.4f}\")\n",
    "\n",
    "            # Total Discriminator Loss\n",
    "            loss_D = (real_loss + fake_loss) * 0.5\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # ============================\n",
    "            # Train Generator\n",
    "            # ============================\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Adversarial loss\n",
    "            pred_fake = discriminator(torch.cat((real_img, fake_img), 1))\n",
    "            loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(DEVICE))\n",
    "\n",
    "            # L1 loss\n",
    "            loss_G_L1 = criterion_L1(fake_img, caricature_img) * L1_LAMBDA\n",
    "\n",
    "            # Total Generator Loss\n",
    "            loss_G = loss_G_GAN + loss_G_L1\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            epoch_loss_G += loss_G.item()\n",
    "            epoch_loss_D += loss_D.item()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{i+1}/{len(train_loader)}], \"\n",
    "                      f\"Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n",
    "\n",
    "        avg_loss_G = epoch_loss_G / len(train_loader)\n",
    "        avg_loss_D = epoch_loss_D / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] completed. Average Loss D: {avg_loss_D:.4f}, Average Loss G: {avg_loss_G:.4f}\")\n",
    "\n",
    "        # ============================\n",
    "        # Validation and Checkpoints\n",
    "        # ============================\n",
    "        generator.eval()\n",
    "        discriminator.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_target in val_loader:\n",
    "                val_input = val_input.to(DEVICE)\n",
    "                fake_val = generator(val_input)\n",
    "                break  # Take first batch\n",
    "\n",
    "        # Denormalize and save images\n",
    "        val_input_denorm = denormalize(val_input.cpu())\n",
    "        fake_val_denorm = denormalize(fake_val.cpu())\n",
    "        val_target_denorm = denormalize(val_target.cpu())\n",
    "\n",
    "        grid_input = torchvision.utils.make_grid(val_input_denorm, nrow=4)\n",
    "        grid_fake = torchvision.utils.make_grid(fake_val_denorm, nrow=4)\n",
    "        grid_target = torchvision.utils.make_grid(val_target_denorm, nrow=4)\n",
    "\n",
    "        torchvision.utils.save_image(grid_input, os.path.join(OUTPUT_DIR, f'epoch_{epoch+1}_input.png'))\n",
    "        torchvision.utils.save_image(grid_fake, os.path.join(OUTPUT_DIR, f'epoch_{epoch+1}_fake.png'))\n",
    "        torchvision.utils.save_image(grid_target, os.path.join(OUTPUT_DIR, f'epoch_{epoch+1}_target.png'))\n",
    "\n",
    "        print(f\"Saved generated images for Epoch {epoch+1}.\")\n",
    "\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), os.path.join(CHECKPOINT_DIR, f'generator_epoch_{epoch+1}.pth'))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(CHECKPOINT_DIR, f'discriminator_epoch_{epoch+1}.pth'))\n",
    "\n",
    "        print(f\"Saved model checkpoints for Epoch {epoch+1}.\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "\n",
    "    # ============================\n",
    "    # Inference Function\n",
    "    # ============================\n",
    "    def generate_caricature(input_image_path, generator, transform, device, output_path):\n",
    "        \"\"\"\n",
    "        Transforms a real image into a caricature using the trained generator.\n",
    "\n",
    "        Args:\n",
    "            input_image_path (str): Path to the input real image.\n",
    "            generator (nn.Module): Trained generator model.\n",
    "            transform (callable): Transformations to apply to the input image.\n",
    "            device (torch.device): Device to perform computations on.\n",
    "            output_path (str): Path to save the generated caricature.\n",
    "        \"\"\"\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Load and preprocess the image\n",
    "                print(f\"Loading test image from {input_image_path}...\")\n",
    "                image = Image.open(input_image_path).convert('RGB')\n",
    "                input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "                print(\"Image loaded and transformed.\")\n",
    "\n",
    "                # Generate caricature\n",
    "                print(\"Generating caricature...\")\n",
    "                fake_tensor = generator(input_tensor)\n",
    "                print(\"Caricature generated.\")\n",
    "\n",
    "                # Denormalize\n",
    "                fake_tensor = denormalize(fake_tensor.cpu())\n",
    "\n",
    "                # Convert to PIL Image\n",
    "                fake_image = fake_tensor.squeeze(0).permute(1, 2, 0).numpy()\n",
    "                fake_image = (fake_image * 255).astype('uint8')\n",
    "                fake_image = Image.fromarray(fake_image)\n",
    "\n",
    "                # Save the image\n",
    "                fake_image.save(output_path)\n",
    "                print(f\"Caricature saved to {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during inference: {e}\")\n",
    "\n",
    "    # ============================\n",
    "    # Example Inference\n",
    "    # ============================\n",
    "    # Uncomment the lines below to perform inference after training\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
